### bytebot-agent-container

[Nest] 17  - 09/16/2025, 10:03:38 PM     LOG ;5;3m[TasksService] Retrieving task by ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:38 PM   DEBUG ;5;3m[TasksService] Retrieved task with ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
Client connected: tEk6D6W95G-WPgq0AAAR
Client tEk6D6W95G-WPgq0AAAR joined task 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM     LOG ;5;3m[TasksService] Found existing task with ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a, and status PENDING. Resuming.
[Nest] 17  - 09/16/2025, 10:03:40 PM     LOG ;5;3m[TasksService] Updating task with ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM   DEBUG ;5;3m[TasksService] Update data: {"status":"RUNNING","executedAt":"2025-09-16T22:03:40.028Z"}
[Nest] 17  - 09/16/2025, 10:03:40 PM     LOG ;5;3m[TasksService] Retrieving task by ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM   DEBUG ;5;3m[TasksService] Retrieved task with ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM     LOG ;5;3m[TasksService] Successfully updated task ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM   DEBUG ;5;3m[TasksService] Updated task: {"id":"1ab05509-c923-4a7d-bfe9-c69674aa1d4a","description":"open the browser","type":"IMMEDIATE","status":"RUNNING","priority":"MEDIUM","control":"ASSISTANT","createdAt":"2025-09-16T22:03:37.930Z","createdBy":"USER","scheduledFor":null,"updatedAt":"2025-09-16T22:03:40.037Z","executedAt":"2025-09-16T22:03:40.028Z","completedAt":null,"queuedAt":null,"error":null,"result":null,"model":{"name":"openrouter/sonoma-sky-alpha","title":"Sonoma Sky Alpha (Reasoning)","provider":"proxy","contextWindow":128000}}
[Nest] 17  - 09/16/2025, 10:03:40 PM   DEBUG ;5;3m[AgentScheduler] Processing task ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM     LOG ;5;3m[AgentProcessor] Starting processing for task ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM     LOG ;5;3m[TasksService] Retrieving task by ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM   DEBUG ;5;3m[TasksService] Retrieved task with ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM     LOG ;5;3m[AgentProcessor] Processing iteration for task ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:40 PM   DEBUG ;5;3m[AgentProcessor] Sending 1 messages to LLM for processing
[Nest] 17  - 09/16/2025, 10:03:43 PM     LOG ;5;3m[TasksService] Updating task with ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:43 PM   DEBUG ;5;3m[TasksService] Update data: {"status":"FAILED"}
[Nest] 17  - 09/16/2025, 10:03:43 PM     LOG ;5;3m[TasksService] Retrieving task by ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:43 PM   DEBUG ;5;3m[TasksService] Retrieved task with ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:43 PM     LOG ;5;3m[TasksService] Successfully updated task ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a
[Nest] 17  - 09/16/2025, 10:03:43 PM   DEBUG ;5;3m[TasksService] Updated task: {"id":"1ab05509-c923-4a7d-bfe9-c69674aa1d4a","description":"open the browser","type":"IMMEDIATE","status":"FAILED","priority":"MEDIUM","control":"ASSISTANT","createdAt":"2025-09-16T22:03:37.930Z","createdBy":"USER","scheduledFor":null,"updatedAt":"2025-09-16T22:03:43.713Z","executedAt":"2025-09-16T22:03:40.028Z","completedAt":null,"queuedAt":null,"error":null,"result":null,"model":{"name":"openrouter/sonoma-sky-alpha","title":"Sonoma Sky Alpha (Reasoning)","provider":"proxy","contextWindow":128000}}
Client disconnected: tEk6D6W95G-WPgq0AAAR
[Nest] 17  - 09/16/2025, 10:03:45 PM     LOG ;5;3m[TasksService] Retrieving tasks - page: 1, limit: 5, statuses: undefined
[Nest] 17  - 09/16/2025, 10:03:45 PM   DEBUG ;5;3m[TasksService] Retrieved 5 tasks out of 27 total
Client connected: XOklWH5-TbPJGqvpAAAT
[Nest] 17  - 09/16/2025, 10:03:45 PM     LOG ;5;3m[TasksService] Retrieving tasks - page: 1, limit: 5, statuses: undefined
[Nest] 17  - 09/16/2025, 10:03:45 PM   DEBUG ;5;3m[TasksService] Retrieved 5 tasks out of 27 total
Client connected: 6My5rKgTjYDuFFo-AAAV
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/thudm/glm-4.1v-9b-thinking
Available Model Group Fallbacks=None
Error: 400 litellm.UnsupportedParamsError: openrouter does not support parameters: ['reasoning_effort'], for model=thudm/glm-4.1v-9b-thinking. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/thudm/glm-4.1v-9b-thinking
Available Model Group Fallbacks=None
    at ProxyService.handleApiError (/app/bytebot-agent/dist/proxy/proxy.service.js:315:31)
    at ProxyService.generateMessage (/app/bytebot-agent/dist/proxy/proxy.service.js:64:40)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async AgentProcessor.runIteration (/app/bytebot-agent/dist/agent/agent.processor.js:137:29)
[Nest] 17  - 09/16/2025, 10:03:43 PM   ERROR ;5;3m[ProxyService] Error sending message to proxy: 400 litellm.UnsupportedParamsError: openrouter does not support parameters: ['reasoning_effort'], for model=sonoma-sky-alpha. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/sonoma-sky-alpha
Available Model Group Fallbacks=None
Error: 400 litellm.UnsupportedParamsError: openrouter does not support parameters: ['reasoning_effort'], for model=sonoma-sky-alpha. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/sonoma-sky-alpha
Available Model Group Fallbacks=None
    at ProxyService.handleApiError (/app/bytebot-agent/dist/proxy/proxy.service.js:315:31)
    at ProxyService.generateMessage (/app/bytebot-agent/dist/proxy/proxy.service.js:64:40)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async AgentProcessor.runIteration (/app/bytebot-agent/dist/agent/agent.processor.js:137:29)
[Nest] 17  - 09/16/2025, 10:03:43 PM   ERROR ;5;3m[AgentProcessor] Error during task processing iteration for task ID: 1ab05509-c923-4a7d-bfe9-c69674aa1d4a - 400 litellm.UnsupportedParamsError: openrouter does not support parameters: ['reasoning_effort'], for model=sonoma-sky-alpha. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/sonoma-sky-alpha
Available Model Group Fallbacks=None
Error: 400 litellm.UnsupportedParamsError: openrouter does not support parameters: ['reasoning_effort'], for model=sonoma-sky-alpha. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/sonoma-sky-alpha
Available Model Group Fallbacks=None
    at ProxyService.handleApiError (/app/bytebot-agent/dist/proxy/proxy.service.js:315:31)
    at ProxyService.generateMessage (/app/bytebot-agent/dist/proxy/proxy.service.js:64:40)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async AgentProcessor.runIteration (/app/bytebot-agent/dist/agent/agent.processor.js:137:29)


### bytebot-llm-proxy-container

INFO:     10.0.16.6:49938 - "POST /chat/completions HTTP/1.1" 400 Bad Request
INFO:     10.0.16.6:49938 - "GET /model/info HTTP/1.1" 200 OK
    )
litellm.exceptions.UnsupportedParamsError: litellm.UnsupportedParamsError: openrouter does not support parameters: ['reasoning_effort'], for model=thudm/glm-4.1v-9b-thinking. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/thudm/glm-4.1v-9b-thinking
Available Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2
22:03:43 - LiteLLM Proxy:ERROR: common_request_processing.py:644 - litellm.proxy.proxy_server._handle_llm_api_exception(): Exception occured - litellm.UnsupportedParamsError: openrouter does not support parameters: ['reasoning_effort'], for model=sonoma-sky-alpha. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/sonoma-sky-alpha
Available Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2
Traceback (most recent call last):
  File "/usr/lib/python3.13/site-packages/litellm/proxy/proxy_server.py", line 4092, in chat_completion
    result = await base_llm_response_processor.base_process_llm_request(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<16 lines>...
    )
    ^
  File "/usr/lib/python3.13/site-packages/litellm/proxy/common_request_processing.py", line 438, in base_process_llm_request
    responses = await llm_responses
                ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 1076, in acompletion
    raise e
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 1052, in acompletion
    response = await self.async_function_with_fallbacks(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 3895, in async_function_with_fallbacks
    return await self.async_function_with_fallbacks_common_utils(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<8 lines>...
    )
    ^
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 3853, in async_function_with_fallbacks_common_utils
    raise original_exception
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 3887, in async_function_with_fallbacks
    response = await self.async_function_with_retries(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 4092, in async_function_with_retries
    raise original_exception
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 3983, in async_function_with_retries
    response = await self.make_call(original_function, *args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 4101, in make_call
    response = await response
               ^^^^^^^^^^^^^^
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 1357, in _acompletion
    raise e
  File "/usr/lib/python3.13/site-packages/litellm/router.py", line 1309, in _acompletion
    response = await _response
               ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.13/site-packages/litellm/utils.py", line 1598, in wrapper_async
    raise e
  File "/usr/lib/python3.13/site-packages/litellm/utils.py", line 1449, in wrapper_async
    result = await original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.13/site-packages/litellm/main.py", line 565, in acompletion
    raise exception_type(
    ...<5 lines>...
    )
  File "/usr/lib/python3.13/site-packages/litellm/main.py", line 538, in acompletion
    init_response = await loop.run_in_executor(None, func_with_context)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.13/concurrent/futures/thread.py", line 59, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/usr/lib/python3.13/site-packages/litellm/utils.py", line 1072, in wrapper
    result = original_function(*args, **kwargs)
  File "/usr/lib/python3.13/site-packages/litellm/main.py", line 3582, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "/usr/lib/python3.13/site-packages/litellm/main.py", line 1248, in completion
    optional_params = get_optional_params(
        **optional_param_args, **non_default_params
    )
  File "/usr/lib/python3.13/site-packages/litellm/utils.py", line 3386, in get_optional_params
    _check_valid_arg(
    ~~~~~~~~~~~~~~~~^
        supported_params=supported_params or [],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/usr/lib/python3.13/site-packages/litellm/utils.py", line 3369, in _check_valid_arg
    raise UnsupportedParamsError(
    ...<2 lines>...
    )
litellm.exceptions.UnsupportedParamsError: litellm.UnsupportedParamsError: openrouter does not support parameters: ['reasoning_effort'], for model=sonoma-sky-alpha. To drop these, set `litellm.drop_params=True` or for proxy:

`litellm_settings:
 drop_params: true`
. 
 If you want to use these params dynamically send allowed_openai_params=['reasoning_effort'] in your request.. Received Model Group=openrouter/sonoma-sky-alpha
Available Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2